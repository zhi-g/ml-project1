{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from costs import compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - (np.dot(tx,w))\n",
    "    N = tx.shape[0]\n",
    "    return (-1 / N) * np.dot(tx.T, e) \n",
    "\n",
    "# TODO check if we have to comply with the project description methods which would mean we have to\n",
    "# get rid of the initial_w param\n",
    "def least_squares_GD(y, tx, initial_w, gamma, max_iters): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        w -= gamma*compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For some reason my code is missing on my file O_O I'll redo it later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    if (tx.ndim == 1):\n",
    "        tx = tx.reshape((tx.shape[0], 1))\n",
    "    \n",
    "    a = np.dot(tx.T, tx)\n",
    "    b = np.dot(tx.T, y)\n",
    "    \n",
    "    # we use solve becaue linalg.inv gives numerical error with big numbers\n",
    "    return np.linalg.solve(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    phi = np.array([])\n",
    "    for i in range(0, x.shape[0]):\n",
    "        for j in range(0, degree+1):\n",
    "            phi = np.append(phi, np.array([x[i] ** j]))\n",
    "\n",
    "    phi = phi.reshape([x.shape[0], degree+1])\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#return both sets as two different variable: train_data, test_data\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    data = np.c_[y, x]\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    total_rows = x.shape[0]\n",
    "    train_rows = int(np.floor(total_rows * ratio))\n",
    "    test_rows = total_rows - train_rows\n",
    "    \n",
    "    #This should be the case, check that I didn't make a mistake above\n",
    "    assert(total_rows == train_rows + test_rows)\n",
    "\n",
    "    train_data = data[:train_rows]\n",
    "    test_data = data[train_rows:(train_rows + test_rows)]\n",
    "    \n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamb):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # Here we assume tx polynomial usually\n",
    "    \n",
    "    # Checking that if tx is of shape (#row,) we make it (#row, 1) for mat muliplication\n",
    "    N = tx.shape[0]\n",
    "    if tx.ndim == 1:\n",
    "        tx = tx.reshape((N,1))\n",
    "    M = tx.shape[1]\n",
    "    \n",
    "    id_mat = np.identity(M)\n",
    "    #Because we don't want to penalize w0 (Asked TA) TODO: check\n",
    "    id_mat[0][0] = 0\n",
    "    \n",
    "    x_inv = np.dot(tx.T, tx)\n",
    "    id_mult = (lamb * (2 * N )) * id_mat #if I copy formula from slides\n",
    "    \n",
    "    #Solve again to compute matrix inverses\n",
    "    a = np.linalg.solve(x_inv + id_mult, np.dot(tx.T, y))\n",
    "    \n",
    "    return a\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
