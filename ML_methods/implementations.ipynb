{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"a function used to compute the loss.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from costs import compute_loss, compute_cost_ll\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - (np.dot(tx,w))\n",
    "    N = tx.shape[0]\n",
    "    return (-1 / N) * np.dot(tx.T, e) \n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        w = w - gamma * compute_gradient(y, tx, w)\n",
    "\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient for batch data.\"\"\"\n",
    "    a = (np.dot(tx, w))\n",
    "    e = y - a\n",
    "    N = len(y)\n",
    "    return (-1 / N) * np.dot(tx.T, e) \n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    batch_size = 10\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        loss = compute_cost(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    if (tx.ndim == 1):\n",
    "        tx = tx.reshape((tx.shape[0], 1))\n",
    "    \n",
    "    a = np.dot(tx.T, tx)\n",
    "    b = np.dot(tx.T, y)\n",
    "    \n",
    "    # we use solve becaue linalg.inv gives numerical error with big numbers\n",
    "    return np.linalg.solve(a,b)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_): # edited this function to return both loss and weights\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    # Here we assume tx polynomial usually\n",
    "    \n",
    "    # Checking that if tx is of shape (#row,) we make it (#row, 1) for mat muliplication\n",
    "    N = tx.shape[0]\n",
    "    if tx.ndim == 1:\n",
    "        tx = tx.reshape((N,1))\n",
    "    M = tx.shape[1]\n",
    "    \n",
    "    id_mat = np.identity(M)\n",
    "    #Because we don't want to penalize w0 (Asked TA) TODO: check\n",
    "    id_mat[0][0] = 0\n",
    "    \n",
    "    x_inv = np.dot(tx.T, tx)\n",
    "    id_mult = (lambda_ * (2 * N )) * id_mat #if I copy formula from slides\n",
    "    \n",
    "    #Solve again to compute matrix inverses\n",
    "    a = np.linalg.solve(x_inv + id_mult, np.dot(tx.T, y))\n",
    "    \n",
    "    return compute_loss(y, tx, a), a \n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iter, gamma ):\n",
    "\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w = initial_w \n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        #loss, w = learning_by_newton_method(y, tx, w, gamma)[0]\n",
    "        loss, grad = calculate_gradient_logistic(y, tx, w)\n",
    "        w = w - gamma * grad\n",
    "        \n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "        \n",
    "    return losses, w\n",
    "    \n",
    "    \n",
    "def reg_logistic_regression(y, tx, lamb, initial_w, max_iter, gamma):\n",
    "    # init parameters\n",
    "    w =  initial_w \n",
    "    threshold = 1e-8\n",
    "    N = tx.shape[0]\n",
    "    losses = []\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        loss, grad = calculate_gradient_logistic(y, tx, w)\n",
    "        \n",
    "        #We don't want to penalize w0 ?sss\n",
    "        m2grad = grad + 2* lamb * w\n",
    "        #m2grad[0] = grad[0]\n",
    "        w = w - gamma * (m2grad)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "       \n",
    "    return losses, w\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
